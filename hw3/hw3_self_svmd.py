import sys

import keras
import numpy as np
import pickle as pickle
import random
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten,Dropout,BatchNormalization,advanced_activations
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator

import tensorflow as tf
import sklearn.utils as ut
tf.python.control_flow_ops = tf
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.1
sess = tf.Session(config = config)
from keras import backend as K
Threshold = 0.999
path = sys.argv[1]
modelname = sys.argv[2]

X_train = np.array(pickle.load(open(path + "all_label.p",'r'))).reshape(5000,3,32,32)
Y_train = []
for i in xrange(0,5000):
	Y_train.append(i/500)
Y_train = np_utils.to_categorical(Y_train, nb_classes=10)
list1_shuf = []
list2_shuf = []
index_shuf = range(len(X_train))
random.shuffle(index_shuf)
for i in index_shuf:
	list1_shuf.append(X_train[i])
	list2_shuf.append(Y_train[i])
X_train = np.array(list1_shuf)
Y_train = np.array(list2_shuf)



Val_X = X_train[4500:len(X_train)-1]
Val_Y = Y_train[4500:len(Y_train)-1]
X_train = X_train[0:4500]
Y_train = Y_train[0:4500]

X_test = pickle.load(open(path + "test.p",'r'))
X_test = np.array(X_test['data'])
X_test = X_test.reshape(10000,3,32,32)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
Val_X = Val_X.astype('float32')
X_train /= 255
X_test /= 255
Val_X /= 255
model = Sequential()

model.add(Convolution2D(
	nb_filter = 64,
	nb_row = 3,
	nb_col = 3,
	border_mode = 'same',    
	input_shape = (3,32,32)
))
model.add(keras.layers.advanced_activations.ELU(alpha=1))
model.add(Convolution2D(64,3,3,border_mode = 'same'))
#model.add(advanced_aci('elu'))
model.add(keras.layers.advanced_activations.ELU(alpha=1))

model.add(BatchNormalization(axis = 1))
model.add(MaxPooling2D(pool_size = (3,3),border_mode = 'same'))
model.add(Dropout(0.3))
model.add(Convolution2D(128,3,3,border_mode = 'same'))
model.add(keras.layers.advanced_activations.ELU(alpha=1))

#model.add(Activation('elu'))
model.add(Convolution2D(128,3,3,border_mode = 'same'))
model.add(keras.layers.advanced_activations.ELU(alpha=1))

#model.add(Activation('elu'))
model.add(BatchNormalization(axis = 1))
model.add(MaxPooling2D(pool_size = (3,3),border_mode = 'same'))
model.add(Dropout(0.3))
model.add(Convolution2D(256,3,3,border_mode = 'same'))
model.add(keras.layers.advanced_activations.ELU(alpha=1))

#model.add(Activation('elu'))
model.add(Convolution2D(256,3,3,border_mode = 'same'))
model.add(keras.layers.advanced_activations.ELU(alpha=1))

#model.add(Activation('elu'))
model.add(BatchNormalization(axis = 1))
model.add(MaxPooling2D(pool_size = (3,3),border_mode = 'same'))
model.add(Dropout(0.3))

model.add(Flatten())
model.add(Dense(512))
model.add(keras.layers.advanced_activations.ELU(alpha=1))
model.add(BatchNormalization(axis = 1))
model.add(Dropout(0.3))
model.add(Dense(10))
model.add(Activation('softmax'))

adam = Adam(lr = 1e-4)
model.compile(optimizer = 'adadelta',loss = 'categorical_crossentropy',metrics = ['accuracy'])


datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images

# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(X_train)

# fit the model on the batches generated by datagen.flow()
model.fit_generator(datagen.flow(X_train, Y_train,
                    batch_size=32),
                    samples_per_epoch=X_train.shape[0],
                    nb_epoch=64,
                    validation_data = (Val_X,Val_Y))

print "import unlabeled data"
unlableData = np.array(pickle.load(open(path + "all_unlabel.p",'r'))).reshape(45000,3,32,32)
unlableData = unlableData.astype('float32')
unlableData /= 255
print " start self training\n"
for i in range(5):
	prob = model.predict_proba(unlableData, batch_size=32, verbose=1)
	print "data predicted\n" 
	argmax_index = np.argmax(prob,axis = 1)
	Use_list = []
	Non_use = []
	for i in range(len(argmax_index)) : 
		if prob[i][argmax_index[i]] > Threshold:
			Use_list.append(i)
		else:	
			Non_use.append(i)
	data = unlableData[Use_list]
        data = np.append(data,X_train,axis = 0)
	Y_uldata =  argmax_index[Use_list]
	Y_uldata = np_utils.to_categorical(Y_uldata, nb_classes=10)
	print len(Y_uldata)
	Y_uldata = np.append(Y_uldata,Y_train,axis = 0)
	print len(Y_uldata)
	Use_list = np.array(Use_list)
	Non_use = np.array(Non_use)

	datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images
	datagen.fit(data)
	# fit the model on the batches generated by datagen.flow()
	model.fit_generator(datagen.flow(data, Y_uldata,
                    batch_size=32),
                    samples_per_epoch=data.shape[0],
                    nb_epoch=15,
                    validation_data = (Val_X,Val_Y))



#print('Training ------------')
# Another way to train the model
#model.fit(X_train, Y_train,shuffle = True, nb_epoch=70, batch_size = 32, validation_split = 0.1)

#out = model.predict(X_test,batch_size = 32 ,verbose = 0)
#print out
model.save(modelname)
